{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines LSTM model setup trained on visual modality data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from data_preparation import prepare_x_data, get_Y_labels, reshape_Y, reshape_X, unscale_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation includes: <br>\n",
    "<ul>\n",
    "  <li>selecting necessary features from source files</li>\n",
    "  <li>creating combined dataset for the model training</li>\n",
    "  <li>reshaping data for model training.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X data - visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = prepare_x_data('../Data/LLDs_video_openface/train',',', 5, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = prepare_x_data('../Data/LLDs_video_openface/dev',',', 5, scaler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y data - YMRS score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_Y_labels('../Data/labels_metadata.csv', 60, 164, scaler)\n",
    "y_train = reshape_Y(y_train,len(x_train),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = get_Y_labels('../Data/labels_metadata.csv', 0, 60, scaler)\n",
    "y_test = reshape_Y(y_test,len(x_test),1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed model for LSTM recurrent neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.LSTM(units = 206, input_shape=(None,465), return_sequences=True))\n",
    "model.add(layers.Dropout(0.004))\n",
    "model.add(layers.LSTM(units = 206, input_shape=(None,206), return_sequences=False))\n",
    "model.add(layers.Dropout(0.004))\n",
    "model.add(layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"mse\", \n",
    "    metrics= [keras.metrics.MeanAbsoluteError()], #['mean_absolute_error'],  \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.003)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model=model, show_dtype=True, show_layer_names=True, show_shapes=True, to_file='LSTM_unimodal_visual.png') # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training setup is based on an iterative approach where model is trained one file at a time, then learned parameters are saved and loaded in the next iterative step. This setup is necessary due to the fact that source files does not have an uniform size and differ in number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = 0\n",
    "\n",
    "train_eval = []\n",
    "train_loss = []\n",
    "train_mae = []\n",
    "\n",
    "for file in x_train:\n",
    "        file = np.array(file).reshape((1, file.shape[0], -1))\n",
    "        model.fit(x=file, \n",
    "                y=y_train[train_index], \n",
    "                epochs=10,\n",
    "                batch_size = len(file))    \n",
    "                \n",
    "        scores = model.evaluate(file, y_train[train_index], verbose = 0)\n",
    "        train_eval.append(scores)\n",
    "        train_loss.append(scores[0])\n",
    "        train_mae.append(scores[1])\n",
    "                \n",
    "        model.save(r'LSTM_train_visual', include_optimizer=True) # Save model configuration to Saved_models.\n",
    "        model = keras.models.load_model(r'LSTM_train_visual') # Load model configuration from Saved_models.\n",
    "                \n",
    "        train_index += 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation - train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = sum(train_loss) / len(train_loss)\n",
    "avg_train_mae = sum(train_mae) / len(train_mae)\n",
    "print(\"Train loss (avg):\", avg_train_loss, \"Train MAE (avg):\", avg_train_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_loss = round(float(scaler.inverse_transform(np.array(avg_train_loss).reshape(-1,1))), 3)\n",
    "real_train_mae = round(float(scaler.inverse_transform(np.array(avg_train_mae).reshape(-1,1))), 3)\n",
    "\n",
    "print(f'AVG MSE: {real_train_loss}')\n",
    "print(f'AVG MAE: {real_train_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Scaled YMRS value\")\n",
    "plt.plot(train_loss, label=\"MSE\")\n",
    "plt.plot(train_mae, label=\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation  - validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset taken from test data is defined as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = x_test[:30], y_test[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = []\n",
    "eval_mae = []\n",
    "eval_index = 0\n",
    "\n",
    "for input in x_val:\n",
    "    input = reshape_X(input)\n",
    "    scores = model.evaluate(input, y_val[eval_index], verbose = 0)\n",
    "    \n",
    "    eval_loss.append(scores[0])\n",
    "    eval_mae.append(scores[1])\n",
    "    \n",
    "    eval_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_eval_loss = sum(eval_loss) / len(eval_loss)\n",
    "avg_eval_mae = sum(eval_mae) / len(eval_mae)\n",
    "print(\"Validation loss (avg):\", avg_eval_loss, \"Validation MAE (avg):\", avg_eval_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_eval_loss = round(float(scaler.inverse_transform(np.array(avg_eval_loss).reshape(-1,1))), 3)\n",
    "real_eval_mae = round(float(scaler.inverse_transform(np.array(avg_eval_mae).reshape(-1,1))), 3)\n",
    "\n",
    "print(f'AVG MSE: {real_eval_loss}')\n",
    "print(f'AVG MAE: {real_eval_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Scaled YMRS value\")\n",
    "plt.plot(eval_loss, label=\"MSE\")\n",
    "plt.plot(eval_mae, label=\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is made on different subset taken from the test dataset. Then actual and predicted YMRS values are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred, y_pred_actual = x_test[30:], y_test[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = []\n",
    "\n",
    "pred_scores = []\n",
    "pred_loss = []\n",
    "pred_mae = []\n",
    "\n",
    "pred_index = 0\n",
    "\n",
    "for file in x_pred:\n",
    "    file = np.array(file).reshape(1, file.shape[0], -1)\n",
    "    pred_y = y_prediction.append(model.predict(file))\n",
    "    \n",
    "    scores = model.evaluate(file, y_pred_actual[pred_index], verbose=0)\n",
    "    pred_scores.append(scores)\n",
    "    pred_loss.append(scores[0])\n",
    "    pred_mae.append(scores[1])\n",
    "    \n",
    "    pred_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation - prediction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pred_loss = sum(pred_loss) / len(pred_loss)\n",
    "avg_pred_mae = sum(pred_mae) / len(pred_mae)\n",
    "print(\"Prediction loss (avg):\", avg_pred_loss, \"Prediction MAE (avg):\", avg_pred_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred_loss = round(float(scaler.inverse_transform(np.array(avg_pred_loss).reshape(-1,1))), 3)\n",
    "real_pred_mae = round(float(scaler.inverse_transform(np.array(avg_pred_mae).reshape(-1,1))), 3)\n",
    "\n",
    "print(f'AVG MSE: {real_pred_loss}')\n",
    "print(f'AVG MAE: {real_pred_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Scaled YMRS value\")\n",
    "plt.plot(pred_loss, label=\"MSE\")\n",
    "plt.plot(pred_mae, label=\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs predicted comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = np.array(y_prediction).reshape(-1, 1)\n",
    "y_pred_actual = np.array(y_pred_actual).reshape(-1,1)\n",
    "\n",
    "y_prediction = unscale_Y(y_prediction, scaler)\n",
    "y_pred_actual = unscale_Y(y_pred_actual, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(data=np.column_stack((y_pred_actual,y_prediction)),columns=['y_actual','y_pred'])\n",
    "pred_df['pred_error'] = pred_df['y_actual'] - pred_df['y_pred']\n",
    "pred_df = pred_df.sort_values(by=['y_actual']).reset_index()\n",
    "pred_df['y_actual'] = pred_df['y_actual'].apply(np.int64)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.plot('y_actual', 'y_pred', kind='scatter')\n",
    "plt.xlabel(\"Actual YMRS\")\n",
    "plt.ylabel(\"Predicted YMRS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.plot('y_actual', 'pred_error', kind='scatter')\n",
    "plt.xlabel(\"Actual YMRS\")\n",
    "plt.ylabel(\"Prediction error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Target value (YMRS)\")\n",
    "plt.plot(pred_df['y_actual'], label=\"Actual\")\n",
    "plt.plot(pred_df['y_pred'], label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7357180ab0aea027c2b76ff246d615b9a5a4459a76ec15419f48cf45c2c81d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
