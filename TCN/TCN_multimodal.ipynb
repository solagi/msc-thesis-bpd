{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from tcn import TCN, compiled_tcn\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, concatenate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from data_preparation import prepare_x_data, get_Y_labels, unscale_Y, reshape_Y, reshape_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation includes: <br>\n",
    "<ul>\n",
    "  <li>selecting necessary features from source files</li>\n",
    "  <li>creating combined dataset for the model training</li>\n",
    "  <li>reshaping data for model training.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_visual = prepare_x_data('../Data/LLDs_video_openface/train',',', 5, scaler)\n",
    "x_train_audio = prepare_x_data('../Data/LLDs_audio_eGeMAPS/train',';', 2, scaler)\n",
    "\n",
    "y_train = get_Y_labels('../Data/labels_metadata.csv', 60, 164, scaler)\n",
    "y_train = reshape_Y(y_train,len(x_train_visual),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_visual = prepare_x_data('../Data/LLDs_video_openface/dev',',', 5, scaler) \n",
    "x_test_audio = prepare_x_data('../Data/LLDs_audio_eGeMAPS/dev',';', 2, scaler)\n",
    "\n",
    "y_test = get_Y_labels('../Data/labels_metadata.csv', 0, 60, scaler)\n",
    "y_test = reshape_Y(y_test,len(x_test_visual),1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_len_dict = {}\n",
    "a_len_dict = {}\n",
    "\n",
    "for idx, df in enumerate(x_train_visual):\n",
    "    v_len_dict.update({idx:len(df)})\n",
    "\n",
    "for idx, df in enumerate(x_train_audio):\n",
    "    a_len_dict.update({idx:len(df)})\n",
    "\n",
    "# Get sequence with highest number of rows for hyper-parameter tuning.\n",
    "v_max_len = max(v_len_dict, key=v_len_dict.get)\n",
    "a_max_len = max(a_len_dict, key=a_len_dict.get)\n",
    "\n",
    "v_max_len = {v_max_len:v_len_dict.get(v_max_len)}\n",
    "a_max_len = {a_max_len:a_len_dict.get(a_max_len)}\n",
    "\n",
    "print(f'Longest seq audio: {a_max_len}')\n",
    "print(f'Longest seq video: {v_max_len}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed model for temporal convolutional neural network architecture for multi-modal dataset. Both data modalities are concatenated together before the last Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer.\n",
    "inputA = Input(shape=(None, 465))\n",
    "inputB = Input(shape=(None, 23))\n",
    "\n",
    "# Define hidden layer.\n",
    "tcn_v = TCN(\n",
    "    nb_filters=213, \n",
    "    kernel_size=35, \n",
    "    dilations=(128,256,512,1024), \n",
    "    padding='causal', \n",
    "    nb_stacks=1, \n",
    "    use_batch_norm=False, \n",
    "    use_layer_norm=False, \n",
    "    use_weight_norm=False, \n",
    "    use_skip_connections=True, \n",
    "    dropout_rate=0.004, \n",
    "    return_sequences=False, \n",
    "    input_shape=(None, 465))(inputA)\n",
    "\n",
    "tcn_a = TCN(\n",
    "    nb_filters=11, \n",
    "    kernel_size=34, \n",
    "    dilations=(128,256,512,1024), \n",
    "    padding='causal',\n",
    "    nb_stacks=1, \n",
    "    use_batch_norm=False, \n",
    "    use_layer_norm=False, \n",
    "    use_weight_norm=False, \n",
    "    use_skip_connections=True,\n",
    "    dropout_rate=0.003, \n",
    "    return_sequences=False, \n",
    "    input_shape=(None, 23))(inputB)\n",
    "\n",
    "\n",
    "# Define output layer.\n",
    "# output_v = Dense(8, activation='relu')(tcn_v)\n",
    "# output_a = Dense(8, activation='relu')(tcn_a)\n",
    "\n",
    "combined_input = concatenate([tcn_v, tcn_a])\n",
    "\n",
    "output = Dense(1, activation='linear')(combined_input)\n",
    "\n",
    "# Define optimizer and show summary.\n",
    "model = Model(inputs=[inputA, inputB], outputs=[output])\n",
    "model.compile(optimizer=Adam(learning_rate=0.003), loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model=model, show_dtype=False, show_layer_names=True, show_shapes=True, to_file='TCN_multi.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training setup is based on an iterative approach where model is trained one file at a time, then learned parameters are saved and loaded in the next iterative step. This setup is necessary due to the fact that source files does not have an uniform size and differ in number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model one file at a time.\n",
    "train_index = 0\n",
    "val_index = 0\n",
    "loss = {}\n",
    "train_eval = []\n",
    "train_loss = []\n",
    "train_mae = []\n",
    "\n",
    "for train_v, train_a in zip(x_train_visual, x_train_audio):\n",
    "    \n",
    "    train_v = np.array(train_v).reshape((1, train_v.shape[0], -1))\n",
    "    train_a = np.array(train_a).reshape((1, train_a.shape[0], -1))\n",
    "\n",
    "    history = model.fit(x=[train_v, train_a], y=y_train[train_index], epochs=10, shuffle=False, verbose=0, callbacks=[early_stop])\n",
    "    loss.update({train_index:history.history})\n",
    "\n",
    "    train_index += 1\n",
    "\n",
    "    # Save model\n",
    "    model.save(r'TCN_Multi', include_optimizer=True)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(r'TCN_Multi', custom_objects={'TCN':TCN})\n",
    "\n",
    "    scores = model.evaluate([train_v, train_a], y_train[val_index], verbose = 0)\n",
    "    train_eval.append(scores)\n",
    "    train_loss.append(scores[0])\n",
    "    train_mae.append(scores[1])\n",
    "\n",
    "    val_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = sum(train_loss) / len(train_loss)\n",
    "avg_train_mae = sum(train_mae) / len(train_mae)\n",
    "print(\"Train loss (avg):\", avg_train_loss, \"Train MAE (avg):\", avg_train_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('AVG MSE Loss')\n",
    "plt.plot(train_loss, label='MSE')\n",
    "plt.plot(train_mae, label='MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation is performed on subset taken from test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_visual, x_val_audio = x_test_visual[:30], x_test_audio[:30]\n",
    "y_val = y_test[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = []\n",
    "eval_mae = []\n",
    "evaluation = []\n",
    "index = 0\n",
    "\n",
    "for input_visual, input_audio in zip(x_val_visual, x_val_audio):\n",
    "    input_visual = reshape_X(input_visual)\n",
    "    input_audio = reshape_X(input_audio)\n",
    "\n",
    "    scores = model.evaluate(\n",
    "        [input_visual,input_audio], \n",
    "        y_val[index],\n",
    "        verbose = 0)\n",
    "\n",
    "    evaluation.append(scores)    \n",
    "    eval_loss.append(scores[0])\n",
    "    eval_mae.append(scores[1])\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_eval_loss = sum(eval_loss) / len(eval_loss)\n",
    "avg_eval_mae = sum(eval_mae) / len(eval_mae)\n",
    "print(\"Validation loss (avg):\", avg_eval_loss, \"Validation MAE (avg):\", avg_eval_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Scaled YMRS value\")\n",
    "plt.plot(eval_loss, label=\"MSE\")\n",
    "plt.plot(eval_mae, label=\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YMRS prediction and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is made on different subset taken from the test dataset. Then actual and predicted YMRS values are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_visual, x_pred_audio = x_test_visual[30:], x_test_audio[30:]\n",
    "y_pred_actual = y_test[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "pred_eval = []\n",
    "pred_loss = []\n",
    "pred_mae = []\n",
    "test_index = 0\n",
    "\n",
    "for test_v, test_a in zip(x_pred_visual, x_pred_audio):\n",
    "    \n",
    "    test_v = np.array(test_v).reshape((1, test_v.shape[0], -1))\n",
    "    test_a = np.array(test_a).reshape((1, test_a.shape[0], -1))\n",
    "\n",
    "    pred = model.predict([test_v, test_a])\n",
    "    prediction.append(pred)\n",
    "\n",
    "    scores = model.evaluate([test_v,test_a], y_pred_actual[test_index], verbose=0)\n",
    "    pred_eval.append(scores)\n",
    "    pred_loss.append(scores[0])\n",
    "    pred_mae.append(scores[1])\n",
    "\n",
    "    test_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pred_loss = sum(pred_loss) / len(pred_loss)\n",
    "avg_pred_mae = sum(pred_mae) / len(pred_mae)\n",
    "print(\"Prediction loss (avg):\", avg_pred_loss, \"Prediction MAE (avg):\", avg_pred_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(prediction).reshape(-1, 1)\n",
    "y_pred_actual = y_pred_actual.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = unscale_Y(prediction, scaler)\n",
    "y_pred_actual = unscale_Y(y_pred_actual, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(data=np.column_stack((y_pred_actual, prediction)), columns=['y_actual','y_pred'])\n",
    "pred_df['pred_error'] = pred_df['y_actual'] - pred_df['y_pred']\n",
    "pred_df = pred_df.sort_values(by=['y_actual']).reset_index()\n",
    "pred_df['y_actual'] = pred_df['y_actual'].apply(np.int64)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot x = actual, y = predicted\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.scatter(pred_df.y_actual, pred_df.y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot x = actual, y = actual - predicted\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Actual - Predicted')\n",
    "plt.scatter(pred_df.y_actual, pred_df.pred_error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Test subject IDs\")\n",
    "plt.ylabel(\"Target value (YMRS)\")\n",
    "plt.plot(pred_df.y_actual, label=\"Actual\")\n",
    "plt.plot(pred_df.y_pred, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7357180ab0aea027c2b76ff246d615b9a5a4459a76ec15419f48cf45c2c81d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
